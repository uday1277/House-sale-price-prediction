{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOy1ePDoTuecDqopeKql4ni",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uday1277/House-sale-price-prediction/blob/main/wine%20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EODmMDbVua2X",
        "outputId": "1452b9fb-defb-4446-e933-27e780b9ca2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Percentage: 67.08333333333333\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "\n",
        "# Split the data into features (X) and target variable (y)\n",
        "X = df.drop('quality', axis=1)\n",
        "y = df['quality']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create a Random Forest model and perform hyperparameter tuning\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=7)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Print accuracy\n",
        "print(f\"Accuracy Percentage: {accuracy_score(y_test, y_pred) * 100}\\n\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.utils import add_self_loops\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "\n",
        "# Assume 'quality' is the target variable\n",
        "X = df.drop('quality', axis=1)\n",
        "y = df['quality']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create a graph data structure using PyTorch Geometric\n",
        "edge_index = add_self_loops(torch.tensor(Data(x=torch.tensor(X_train_scaled, dtype=torch.float)).edge_index)[0])\n",
        "\n",
        "# Define a simple GNN model using PyTorch Geometric\n",
        "class GNNModel(MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GNNModel, self).__init__(aggr='add')\n",
        "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.lin(x)\n",
        "        return self.propagate(edge_index, x=x)\n",
        "\n",
        "    def message(self, x_j, edge_index, size):\n",
        "        return x_j\n",
        "\n",
        "    def update(self, aggr_out):\n",
        "        return aggr_out\n",
        "\n",
        "# Initialize and train the GNN model\n",
        "gnn_model = GNNModel(in_channels=X_train_scaled.shape[1], out_channels=1)\n",
        "optimizer = torch.optim.Adam(gnn_model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(100):\n",
        "    gnn_model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = gnn_model(x=torch.tensor(X_train_scaled, dtype=torch.float), edge_index=edge_index)\n",
        "    loss = F.binary_cross_entropy_with_logits(out, torch.tensor(y_train.values, dtype=torch.float).view(-1, 1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Evaluate the GNN model\n",
        "gnn_model.eval()\n",
        "with torch.no_grad():\n",
        "    x_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float)\n",
        "    gnn_predictions = gnn_model(x=x_test_tensor, edge_index=edge_index).round()\n",
        "\n",
        "gnn_accuracy = accuracy_score(y_test, gnn_predictions)\n",
        "print(f\"GNN Accuracy: {gnn_accuracy * 100}\\n\\n\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "ShKZNIpWfUKu",
        "outputId": "f16f3caf-bac0-412e-e83b-a4613595a702"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-950ce08324a3>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMessagePassing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madd_self_loops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_geometric'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bE3dPNn6fsXi",
        "outputId": "1786668d-0615-407d-b4cc-fe7198cdbf56"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting data\n",
            "  Downloading data-0.4.tar.gz (7.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from data) (1.16.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from data) (4.4.2)\n",
            "Collecting funcsigs (from data)\n",
            "  Downloading funcsigs-1.0.2-py2.py3-none-any.whl (17 kB)\n",
            "Building wheels for collected packages: data\n",
            "  Building wheel for data (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for data: filename=data-0.4-py3-none-any.whl size=7227 sha256=ae0d0530c1e5def7d7657461c256ddf0890cc42655350d5e48c4fca4d74d86d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/e8/fa/e253c256048ea58d99a8abb5e751abb6a838af6f12887b5418\n",
            "Successfully built data\n",
            "Installing collected packages: funcsigs, data\n",
            "Successfully installed data-0.4 funcsigs-1.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install MessagePassing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3iyAhf0f0rE",
        "outputId": "59bf45da-2336-47fc-844a-aebeac7a8d5c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement MessagePassing (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for MessagePassing\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install add_self_loops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMsmipH5f_EZ",
        "outputId": "9d3ab361-dd0b-4909-83c3-4a4a69baec3c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement add_self_loops (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for add_self_loops\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric.nn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SF4LH_1HgCoI",
        "outputId": "dcf636d8-1978-4423-a1ad-d42ada3bcfc2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch_geometric.nn (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch_geometric.nn\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch import nn, optim\n",
        "\n",
        "# Load and pre-process data\n",
        "wine_data = pd.read_csv(\"dataset.csv\", sep=\";\")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_scaled, labels, test_size=0.2)\n",
        "\n",
        "# Define adjacency matrix (replace with your actual adjacency matrix construction)\n",
        "adj_matrix = ...  # Fill this with your adjacency matrix generation logic\n",
        "\n",
        "# Define GCN architecture\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features):\n",
        "        super(GCN, self).__init__()\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.fc2 = nn.Linear(hidden_features, 1)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        # Graph convolution layer\n",
        "        h = self.fc1(x)\n",
        "        h = torch.matmul(adj, h)\n",
        "        h = torch.relu(h)\n",
        "        # Output layer\n",
        "        out = self.fc2(h)\n",
        "        return out\n",
        "\n",
        "# Build and train GCN\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# Convert data to DataLoader for GCN\n",
        "train_dataset = TensorDataset(torch.tensor(X_train), torch.tensor(adj_matrix))\n",
        "train_loader = DataLoader(train_dataset, batch_size=32)\n",
        "\n",
        "for epoch in range(100):\n",
        "    for x, adj in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(x, adj)\n",
        "        loss = loss_fn(y_pred, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluate GCN\n",
        "y_pred_gcn = model(torch.tensor(X_test), torch.tensor(adj_matrix))\n",
        "\n",
        "# Build and train Decision Tree\n",
        "dt_model = RandomForestRegressor()\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate Decision Tree\n",
        "y_pred_dt = dt_model.predict(X_test)\n",
        "\n",
        "# Compare performance\n",
        "mse_gcn = nn.MSELoss()(y_pred_gcn, y_test).item()\n",
        "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
        "print(\"GCN MSE:\", mse_gcn)\n",
        "print(\"Decision Tree MSE:\", mse_dt)\n",
        "\n",
        "# Further analysis... (e.g., visualization, parameter analysis)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "53q-7WXvgiub",
        "outputId": "2a51d90a-13ce-4637-9f8b-df854c69468d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-f6fc88e36bfd>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Split data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Define adjacency matrix (replace with your actual adjacency matrix construction)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'features_scaled' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from torch_geometric.nn import GCNConv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "\n",
        "# Assume 'quality' is the target variable\n",
        "X = df.drop('quality', axis=1)\n",
        "y = df['quality']\n",
        "\n",
        "# Map target variable to start from 0\n",
        "y_mapping = {label: idx for idx, label in enumerate(sorted(y.unique()))}\n",
        "y = y.map(y_mapping)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "x_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "x_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float)\n",
        "\n",
        "# Create a graph data structure using PyTorch Geometric\n",
        "edge_index = torch.tensor(Data(x=x_train_tensor).edge_index)\n",
        "\n",
        "# Define a simple Graph Convolutional Network (GCN) model\n",
        "class GNNModel(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GNNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Initialize and train the GNN model\n",
        "model = GNNModel(in_channels=X_train_scaled.shape[1], hidden_channels=64, out_channels=len(y_mapping))\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Convert data to PyTorch Geometric Data object\n",
        "data = Data(x=x_train_tensor, edge_index=edge_index, y=y_train_tensor)\n",
        "loader = DataLoader([data], batch_size=1, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(100):\n",
        "    model.train()\n",
        "    for batch in loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch)\n",
        "        loss = criterion(out, batch.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluate the GNN model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    out = model(Data(x=x_test_tensor, edge_index=edge_index))\n",
        "    y_pred = out.argmax(dim=1).numpy()\n",
        "\n",
        "# Reverse map predicted values to original labels\n",
        "y_pred_original = pd.Series(y_pred).map({v: k for k, v in y_mapping.items()})\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred_original)\n",
        "print(f\"Accuracy Percentage: {accuracy * 100}\\n\\n\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "e91fxKuZj-iS",
        "outputId": "b1266765-073e-46c9-9ce7-10922203547c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-673949adf60f>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGCNConv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_geometric'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-geometric\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rpt9LK0XlI1O",
        "outputId": "2a2617ce-273e-40ea-b23e-b8b4b12a4525"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2023.11.17)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.2.0)\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pDoJmeJlPKp",
        "outputId": "b22e1ebc-5cf6-4703-b658-4045b34663cf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from torch_geometric.nn import GCNConv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Try to read the CSV file\n",
        "try:\n",
        "    df = pd.read_csv(\"dataset.csv\", encoding='latin1')\n",
        "    print(\"CSV file read successfully.\")\n",
        "except pd.errors.ParserError as e:\n",
        "    print(f\"Error reading CSV file: {e}\")\n",
        "    # Handle the error or exit the script\n",
        "\n",
        "# Check the first few rows of the DataFrame\n",
        "print(df.head())\n",
        "\n",
        "# Assuming 'quality' is the target variable\n",
        "# Map target variable to start from 0\n",
        "y_mapping = {label: idx for idx, label in enumerate(sorted(df['quality'].unique()))}\n",
        "df['quality'] = df['quality'].map(y_mapping)\n",
        "\n",
        "# Split the data into features (X) and target variable (y)\n",
        "X = df.drop('quality', axis=1)\n",
        "y = df['quality']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "x_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "x_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float)\n",
        "\n",
        "# Create a graph data structure using PyTorch Geometric\n",
        "num_nodes = X_train_scaled.shape[0]\n",
        "edge_index = torch.tensor([[i, i] for i in range(num_nodes)], dtype=torch.long).t().contiguous()\n",
        "\n",
        "# Define a simple Graph Convolutional Network (GCN) model\n",
        "class SimpleGNNModel(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(SimpleGNNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Initialize and train the GNN model\n",
        "model = SimpleGNNModel(in_channels=X_train_scaled.shape[1], out_channels=len(y_mapping))\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Convert data to PyTorch Geometric Data object\n",
        "data = Data(x=x_train_tensor, edge_index=edge_index, y=y_train_tensor)\n",
        "loader = DataLoader([data], batch_size=1, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(200):\n",
        "    model.train()\n",
        "    for batch in loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch)\n",
        "        loss = criterion(out, batch.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluate the GNN model on the test set\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_data = Data(x=x_test_tensor, edge_index=edge_index)\n",
        "    out = model(test_data)\n",
        "    y_pred = out.argmax(dim=1).numpy()\n",
        "\n",
        "# Reverse map predicted values to original labels\n",
        "y_pred_original = pd.Series(y_pred).map({v: k for k, v in y_mapping.items()})\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred_original)\n",
        "print(f\"Accuracy Percentage: {accuracy * 100}\\n\\n\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUUVEoFHlVjq",
        "outputId": "23fa8284-d366-4bc5-c224-c258d00353cc"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error reading CSV file: Error tokenizing data. C error: Expected 1 fields in line 4, saw 3\n",
            "\n",
            "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
            "0            7.4              0.70         0.00             1.9      0.076   \n",
            "1            7.8              0.88         0.00             2.6      0.098   \n",
            "2            7.8              0.76         0.04             2.3      0.092   \n",
            "3           11.2              0.28         0.56             1.9      0.075   \n",
            "4            7.4              0.70         0.00             1.9      0.076   \n",
            "\n",
            "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
            "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
            "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
            "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
            "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
            "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
            "\n",
            "   alcohol  quality  \n",
            "0      9.4        5  \n",
            "1      9.8        5  \n",
            "2      9.8        5  \n",
            "3      9.8        6  \n",
            "4      9.4        5  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Percentage: 0.0\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Try to read the CSV file\n",
        "try:\n",
        "    df = pd.read_csv(\"dataset.csv\", encoding='latin1')\n",
        "    print(\"CSV file read successfully.\")\n",
        "except pd.errors.ParserError as e:\n",
        "    print(f\"Error reading CSV file: {e}\")\n",
        "    # Handle the error or exit the script\n",
        "\n",
        "# Check the first few rows of the DataFrame\n",
        "print(df.head())\n",
        "\n",
        "# Assuming 'quality' is the target variable\n",
        "# Map target variable to start from 0\n",
        "y_mapping = {label: idx for idx, label in enumerate(sorted(df['quality'].unique()))}\n",
        "df['quality'] = df['quality'].map(y_mapping)\n",
        "\n",
        "# Split the data into features (X) and target variable (y)\n",
        "X = df.drop('quality', axis=1)\n",
        "y = df['quality']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create an XGBoost model\n",
        "model = XGBClassifier(random_state=7)\n",
        "\n",
        "# Fit the model to the training data\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Reverse map predicted values to original labels\n",
        "y_pred_original = pd.Series(y_pred).map({v: k for k, v in y_mapping.items()})\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred_original)\n",
        "print(f\"Accuracy Percentage: {accuracy * 100}\\n\\n\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CDTHa2Rp4Wj",
        "outputId": "6417b788-3a7c-428d-b9a6-b2850a25499b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error reading CSV file: Error tokenizing data. C error: Expected 1 fields in line 4, saw 3\n",
            "\n",
            "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
            "0            7.4              0.70         0.00             1.9      0.076   \n",
            "1            7.8              0.88         0.00             2.6      0.098   \n",
            "2            7.8              0.76         0.04             2.3      0.092   \n",
            "3           11.2              0.28         0.56             1.9      0.075   \n",
            "4            7.4              0.70         0.00             1.9      0.076   \n",
            "\n",
            "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
            "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
            "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
            "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
            "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
            "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
            "\n",
            "   alcohol  quality  \n",
            "0      9.4        2  \n",
            "1      9.8        2  \n",
            "2      9.8        2  \n",
            "3      9.8        3  \n",
            "4      9.4        2  \n",
            "Accuracy Percentage: 65.83333333333333\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Try to read the CSV file\n",
        "try:\n",
        "    df = pd.read_csv(\"dataset.csv\", encoding='latin1')\n",
        "    print(\"CSV file read successfully.\")\n",
        "except pd.errors.ParserError as e:\n",
        "    print(f\"Error reading CSV file: {e}\")\n",
        "    # Handle the error or exit the script\n",
        "\n",
        "# Check the first few rows of the DataFrame\n",
        "print(df.head())\n",
        "\n",
        "# Assuming 'quality' is the target variable\n",
        "# Map target variable to start from 0\n",
        "y_mapping = {label: idx for idx, label in enumerate(sorted(df['quality'].unique()))}\n",
        "df['quality'] = df['quality'].map(y_mapping)\n",
        "\n",
        "# Split the data into features (X) and target variable (y)\n",
        "X = df.drop('quality', axis=1)\n",
        "y = df['quality']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create an XGBoost model\n",
        "model = XGBClassifier(random_state=7)\n",
        "\n",
        "# Fit the model to the training data\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Reverse map predicted values to original labels\n",
        "y_pred_original = pd.Series(y_pred).map({v: k for k, v in y_mapping.items()})\n",
        "\n",
        "# Print unique values of y_test and y_pred_original for debugging\n",
        "print(\"Unique values in y_test:\", y_test.unique())\n",
        "print(\"Unique values in y_pred_original:\", y_pred_original.unique())\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred_original)\n",
        "print(f\"Accuracy Percentage: {accuracy * 100}\\n\\n\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rJAhyDssFUQ",
        "outputId": "05817c9a-4f4e-4a41-8231-2a300bef4c64"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error reading CSV file: Error tokenizing data. C error: Expected 1 fields in line 4, saw 3\n",
            "\n",
            "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
            "0            7.4              0.70         0.00             1.9      0.076   \n",
            "1            7.8              0.88         0.00             2.6      0.098   \n",
            "2            7.8              0.76         0.04             2.3      0.092   \n",
            "3           11.2              0.28         0.56             1.9      0.075   \n",
            "4            7.4              0.70         0.00             1.9      0.076   \n",
            "\n",
            "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
            "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
            "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
            "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
            "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
            "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
            "\n",
            "   alcohol  quality  \n",
            "0      9.4        2  \n",
            "1      9.8        2  \n",
            "2      9.8        2  \n",
            "3      9.8        3  \n",
            "4      9.4        2  \n",
            "Unique values in y_test: [3 2 4 1 5 0]\n",
            "Unique values in y_pred_original: [2 3 4 1 5 0]\n",
            "Accuracy Percentage: 65.83333333333333\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from torch_geometric.nn import GCNConv\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Try to read the CSV file\n",
        "try:\n",
        "    df = pd.read_csv(\"dataset.csv\", encoding='latin1')\n",
        "    print(\"CSV file read successfully.\")\n",
        "except pd.errors.ParserError as e:\n",
        "    print(f\"Error reading CSV file: {e}\")\n",
        "    # Handle the error or exit the script\n",
        "\n",
        "# Assuming 'quality' is the target variable\n",
        "# Map target variable to start from 0\n",
        "y_mapping = {label: idx for idx, label in enumerate(sorted(df['quality'].unique()))}\n",
        "df['quality'] = df['quality'].map(y_mapping)\n",
        "\n",
        "# Split the data into features (X) and target variable (y)\n",
        "X = df.drop('quality', axis=1)\n",
        "y = df['quality']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "x_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "x_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float)\n",
        "\n",
        "# Create a graph data structure using PyTorch Geometric\n",
        "num_nodes = X_train_scaled.shape[0]\n",
        "edge_index = torch.tensor([[i, i] for i in range(num_nodes)], dtype=torch.long).t().contiguous()\n",
        "\n",
        "# Define a simple Graph Convolutional Network (GCN) model\n",
        "class SimpleGNNModel(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(SimpleGNNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Initialize and train the GNN model\n",
        "model = SimpleGNNModel(in_channels=X_train_scaled.shape[1], out_channels=len(y_mapping))\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Convert data to PyTorch Geometric Data object\n",
        "data = Data(x=x_train_tensor, edge_index=edge_index, y=y_train_tensor)\n",
        "loader = DataLoader([data], batch_size=1, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(200):\n",
        "    model.train()\n",
        "    for batch in loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch)\n",
        "        loss = criterion(out, batch.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluate the GNN model on the test set\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_data = Data(x=x_test_tensor, edge_index=edge_index)\n",
        "    out = model(test_data)\n",
        "    y_pred = out.argmax(dim=1).numpy()\n",
        "\n",
        "# Reverse map predicted values to original labels\n",
        "y_pred_original = pd.Series(y_pred).map({v: k for k, v in y_mapping.items()})\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred_original)\n",
        "print(f\"Accuracy Percentage: {accuracy * 100}\\n\\n\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fkzRMjfsa9a",
        "outputId": "ee35bad3-1362-4c19-eac5-06027a651d40"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error reading CSV file: Error tokenizing data. C error: Expected 1 fields in line 4, saw 3\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Percentage: 57.91666666666667\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Try to read the CSV file\n",
        "try:\n",
        "    df = pd.read_csv(\"dataset.csv\", encoding='latin1')\n",
        "    print(\"CSV file read successfully.\")\n",
        "except pd.errors.ParserError as e:\n",
        "    print(f\"Error reading CSV file: {e}\")\n",
        "    # Handle the error or exit the script\n",
        "\n",
        "# Assuming 'quality' is the target variable\n",
        "# Map target variable to start from 0\n",
        "y_mapping = {label: idx for idx, label in enumerate(sorted(df['quality'].unique()))}\n",
        "df['quality'] = df['quality'].map(y_mapping)\n",
        "\n",
        "# Split the data into features (X) and target variable (y)\n",
        "X = df.drop('quality', axis=1)\n",
        "y = df['quality']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create a Decision Tree model\n",
        "model = DecisionTreeClassifier(random_state=7)\n",
        "\n",
        "# Fit the model to the training data\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Reverse map predicted values to original labels\n",
        "y_pred_original = pd.Series(y_pred).map({v: k for k, v in y_mapping.items()})\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred_original)\n",
        "print(f\"Accuracy Percentage: {accuracy * 100}\\n\\n\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEgEPYcLsw46",
        "outputId": "d3904b20-692b-450b-cb24-1b21101b01fc"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error reading CSV file: Error tokenizing data. C error: Expected 1 fields in line 4, saw 3\n",
            "\n",
            "Accuracy Percentage: 59.166666666666664\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from torch_geometric.nn import ChebConv\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Try to read the CSV file\n",
        "try:\n",
        "    df = pd.read_csv(\"dataset.csv\", encoding='latin1')\n",
        "    print(\"CSV file read successfully.\")\n",
        "except pd.errors.ParserError as e:\n",
        "    print(f\"Error reading CSV file: {e}\")\n",
        "    # Handle the error or exit the script\n",
        "\n",
        "# Assuming 'quality' is the target variable\n",
        "# Map target variable to start from 0\n",
        "y_mapping = {label: idx for idx, label in enumerate(sorted(df['quality'].unique()))}\n",
        "df['quality'] = df['quality'].map(y_mapping)\n",
        "\n",
        "# Split the data into features (X) and target variable (y)\n",
        "X = df.drop('quality', axis=1)\n",
        "y = df['quality']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "x_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "x_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float)\n",
        "\n",
        "# Create a graph data structure using PyTorch Geometric\n",
        "num_nodes = X_train_scaled.shape[0]\n",
        "edge_index = torch.tensor([[i, i] for i in range(num_nodes)], dtype=torch.long).t().contiguous()\n",
        "\n",
        "# Define an improved Graph Convolutional Network (GCN) model\n",
        "class ImprovedGNNModel(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, dropout_rate=0.5):\n",
        "        super(ImprovedGNNModel, self).__init__()\n",
        "        self.conv1 = ChebConv(in_channels, hidden_channels, K=2)\n",
        "        self.conv2 = ChebConv(hidden_channels, out_channels, K=2)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Initialize and train the improved GNN model\n",
        "improved_model = ImprovedGNNModel(\n",
        "    in_channels=X_train_scaled.shape[1],\n",
        "    hidden_channels=64,\n",
        "    out_channels=len(y_mapping),\n",
        "    dropout_rate=0.5\n",
        ")\n",
        "optimizer = torch.optim.Adam(improved_model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Convert data to PyTorch Geometric Data object\n",
        "data = Data(x=x_train_tensor, edge_index=edge_index, y=y_train_tensor)\n",
        "loader = DataLoader([data], batch_size=1, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(200):\n",
        "    improved_model.train()\n",
        "    for batch in loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = improved_model(batch)\n",
        "        loss = criterion(out, batch.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluate the GNN model on the test set\n",
        "improved_model.eval()\n",
        "with torch.no_grad():\n",
        "    test_data = Data(x=x_test_tensor, edge_index=edge_index)\n",
        "    out = improved_model(test_data)\n",
        "    y_pred = out.argmax(dim=1).numpy()\n",
        "\n",
        "# Reverse map predicted values to original labels\n",
        "y_pred_original = pd.Series(y_pred).map({v: k for k, v in y_mapping.items()})\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred_original)\n",
        "print(f\"Accuracy Percentage: {accuracy * 100}\\n\\n\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAc0Z7WWtLVt",
        "outputId": "4763e26b-1b9d-4c6b-98ec-ea6397057bfb"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error reading CSV file: Error tokenizing data. C error: Expected 1 fields in line 4, saw 3\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Percentage: 59.166666666666664\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from torch_geometric.nn import ChebConv\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Try to read the CSV file\n",
        "try:\n",
        "    df = pd.read_csv(\"dataset.csv\", encoding='latin1')\n",
        "    print(\"CSV file read successfully.\")\n",
        "except pd.errors.ParserError as e:\n",
        "    print(f\"Error reading CSV file: {e}\")\n",
        "    # Handle the error or exit the script\n",
        "\n",
        "# Assuming 'quality' is the target variable\n",
        "# Map target variable to start from 0\n",
        "y_mapping = {label: idx for idx, label in enumerate(sorted(df['quality'].unique()))}\n",
        "df['quality'] = df['quality'].map(y_mapping)\n",
        "\n",
        "# Split the data into features (X) and target variable (y)\n",
        "X = df.drop('quality', axis=1)\n",
        "y = df['quality']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "x_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "x_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float)\n",
        "\n",
        "# Create a graph data structure using PyTorch Geometric\n",
        "num_nodes = X_train_scaled.shape[0]\n",
        "edge_index = torch.tensor([[i, i] for i in range(num_nodes)], dtype=torch.long).t().contiguous()\n",
        "\n",
        "# Define an improved Graph Convolutional Network (GCN) model\n",
        "class ImprovedGNNModel(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels1, hidden_channels2, out_channels, dropout_rate=0.5):\n",
        "        super(ImprovedGNNModel, self).__init__()\n",
        "        self.conv1 = ChebConv(in_channels, hidden_channels1, K=2)\n",
        "        self.conv2 = ChebConv(hidden_channels1, hidden_channels2, K=2)\n",
        "        self.conv3 = ChebConv(hidden_channels2, out_channels, K=2)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.conv3(x, edge_index))\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Initialize and train the improved GNN model\n",
        "improved_model = ImprovedGNNModel(\n",
        "    in_channels=X_train_scaled.shape[1],\n",
        "    hidden_channels1=64,\n",
        "    hidden_channels2=32,\n",
        "    out_channels=len(y_mapping),\n",
        "    dropout_rate=0.5\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.Adam(improved_model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Convert data to PyTorch Geometric Data object\n",
        "data = Data(x=x_train_tensor, edge_index=edge_index, y=y_train_tensor)\n",
        "loader = DataLoader([data], batch_size=1, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(200):\n",
        "    improved_model.train()\n",
        "    for batch in loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = improved_model(batch)\n",
        "        loss = criterion(out, batch.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluate the improved GNN model on the test set\n",
        "improved_model.eval()\n",
        "with torch.no_grad():\n",
        "    test_data = Data(x=x_test_tensor, edge_index=edge_index)\n",
        "    out = improved_model(test_data)\n",
        "    y_pred = out.argmax(dim=1).numpy()\n",
        "\n",
        "# Reverse map predicted values to original labels\n",
        "y_pred_original = pd.Series(y_pred).map({v: k for k, v in y_mapping.items()})\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred_original)\n",
        "print(f\"Accuracy Percentage: {accuracy * 100}\\n\\n\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_biGfKF4t1kj",
        "outputId": "d4c08fe6-7297-40ca-b497-440e78aba164"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error reading CSV file: Error tokenizing data. C error: Expected 1 fields in line 4, saw 3\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Percentage: 61.04166666666667\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Try to read the CSV file\n",
        "try:\n",
        "    df = pd.read_csv(\"dataset.csv\", encoding='latin1')\n",
        "    print(\"CSV file read successfully.\")\n",
        "except pd.errors.ParserError as e:\n",
        "    print(f\"Error reading CSV file: {e}\")\n",
        "    # Handle the error or exit the script\n",
        "\n",
        "# Assuming 'quality' is the target variable\n",
        "# Map target variable to start from 0\n",
        "y_mapping = {label: idx for idx, label in enumerate(sorted(df['quality'].unique()))}\n",
        "df['quality'] = df['quality'].map(y_mapping)\n",
        "\n",
        "# Split the data into features (X) and target variable (y)\n",
        "X = df.drop('quality', axis=1)\n",
        "y = df['quality']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create a KNN model\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Fit the model to the training data\n",
        "knn_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = knn_model.predict(X_test_scaled)\n",
        "\n",
        "# Reverse map predicted values to original labels\n",
        "y_pred_original = pd.Series(y_pred).map({v: k for k, v in y_mapping.items()})\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred_original)\n",
        "print(f\"Accuracy Percentage: {accuracy * 100}\\n\\n\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AU7hgHhuWos",
        "outputId": "396697a6-02ca-4d80-ca03-6be247298f6e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error reading CSV file: Error tokenizing data. C error: Expected 1 fields in line 4, saw 3\n",
            "\n",
            "Accuracy Percentage: 58.75\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LeBugooNo8iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Try to read the CSV file\n",
        "try:\n",
        "    df = pd.read_csv(\"dataset.csv\", encoding='latin1')\n",
        "    print(\"CSV file read successfully.\")\n",
        "except pd.errors.ParserError as e:\n",
        "    print(f\"Error reading CSV file: {e}\")\n",
        "    # Handle the error or exit the script\n",
        "\n",
        "# Assuming 'quality' is the target variable\n",
        "# Map target variable to start from 0\n",
        "y_mapping = {label: idx for idx, label in enumerate(sorted(df['quality'].unique()))}\n",
        "df['quality'] = df['quality'].map(y_mapping)\n",
        "\n",
        "# Split the data into features (X) and target variable (y)\n",
        "X = df.drop('quality', axis=1)\n",
        "y = df['quality']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create an SVM model\n",
        "svm_model = SVC(kernel='rbf', C=1.0, random_state=7)\n",
        "\n",
        "# Fit the model to the training data\n",
        "svm_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svm_model.predict(X_test_scaled)\n",
        "\n",
        "# Reverse map predicted values to original labels\n",
        "y_pred_original = pd.Series(y_pred).map({v: k for k, v in y_mapping.items()})\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred_original)\n",
        "print(f\"Accuracy Percentage: {accuracy * 100}\\n\\n\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YL0JYEzvSoR",
        "outputId": "f8b94317-6c22-42cd-f11a-030906938e41"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error reading CSV file: Error tokenizing data. C error: Expected 1 fields in line 4, saw 3\n",
            "\n",
            "Accuracy Percentage: 61.04166666666667\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Try to read the CSV file\n",
        "try:\n",
        "    df = pd.read_csv(\"dataset.csv\", encoding='latin1')\n",
        "    print(\"CSV file read successfully.\")\n",
        "except pd.errors.ParserError as e:\n",
        "    print(f\"Error reading CSV file: {e}\")\n",
        "    # Handle the error or exit the script\n",
        "\n",
        "# Assuming 'quality' is the target variable\n",
        "# Map target variable to start from 0\n",
        "y_mapping = {label: idx for idx, label in enumerate(sorted(df['quality'].unique()))}\n",
        "df['quality'] = df['quality'].map(y_mapping)\n",
        "\n",
        "# Split the data into features (X) and target variable (y)\n",
        "X = df.drop('quality', axis=1)\n",
        "y = df['quality']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "logistic_model = LogisticRegression(random_state=7)\n",
        "\n",
        "# Fit the model to the training data\n",
        "logistic_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = logistic_model.predict(X_test_scaled)\n",
        "\n",
        "# Reverse map predicted values to original labels\n",
        "y_pred_original = pd.Series(y_pred).map({v: k for k, v in y_mapping.items()})\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred_original)\n",
        "print(f\"Accuracy Percentage: {accuracy * 100}\\n\\n\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m00B0qowveGz",
        "outputId": "6c8bacda-aa2b-4984-ae90-7f17f0741adb"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error reading CSV file: Error tokenizing data. C error: Expected 1 fields in line 4, saw 3\n",
            "\n",
            "Accuracy Percentage: 57.70833333333333\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}